Name,Date,AuthorName,Venue,BibTeX,Notes,Compares To,Models,Datasets,Non-pruning,Results
local-competition,2019-6,Panousis,ICML,"@article{panousis2018nonparametric,
  title={Nonparametric Bayesian Deep Networks with Local Competition},
  author={Panousis, Konstantinos P and Chatzis, Sotirios and Theodoridis, Sergios},
  journal={arXiv preprint arXiv:1805.07624},
  year={2018}
}",,"structured-log-normal,sparse-variational-dropout,bayesian-compression,dai-info-bottleneck","LeNet-300-100, LeNet-5-Caffe, conv64-64-fc384-192","MNIST, MNIST, CIFAR-10",,"356,357,358"
peng-collaborative,2019-6,Peng,ICML,"@inproceedings{peng2019collaborative,
  title={Collaborative Channel Pruning for Deep Networks},
  author={Peng, Hanyu and Wu, Jiaxiang and Chen, Shifeng and Huang, Junzhou},
  booktitle={International Conference on Machine Learning},
  pages={5113--5122},
  year={2019}
}",,"channel-lasso-lstsq,pruning-filters,thinet-channel-norms,soft-filter-pruning,nisp,amc-automl-han,zhuang-discriminative-channel","ResNet-56, ResNet-50, ResNet-50, ResNet-50, ResNet-56, ResNet-56","CIFAR-10, ImageNet, ImageNet, ImageNet, CIFAR-10, CIFAR-10",,"359,360,434,435,436,437"
eigenDamage,2019-6,Wang,ICML,"@article{wang2019eigendamage,
  title={EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis},
  author={Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  journal={arXiv preprint arXiv:1905.05934},
  year={2019}
}",,"optimal-brain-damage,optimal-brain-surgeon,network-slimming","VGG-19, VGG-19+L1, ResNet-32, PreResNet-29+L1, VGG-19, VGG-19+L1, ResNet-32, PreResNet-29+L1, VGG-19","CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-100, CIFAR-100, CIFAR-100, CIFAR-100, Tiny ImageNet",,"347,348,349,350,351,352,353,354,355"
SNIP,2019-5,Lee,ICLR,"@article{lee2018snip,
  title={SNIP: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}",,"learning-both,net-surgery,learning-compression,more-is-less,optimal-brain-damage,sparse-variational-dropout,welling-slow-quantize+prune","LeNet-300-100, LeNet-5, AlexNet-s, AlexNet-b, VGG-C+BN, VGG-D+BN, VGG-like, WRN-16-8, WRN-16-10, WRN-22-8, LSTM-s, LSTM-b, GRU-s, GRU-b, AlexNet-s, AlexNet-b, VGG-C+BN, VGG-D+BN, VGG-like","MNIST, MNIST, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, Sequential MNIST, Sequential MNIST, Sequential MNIST, Sequential MNIST, Tiny ImageNet, Tiny ImageNet, Tiny ImageNet, Tiny ImageNet, Tiny ImageNet",,"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19"
samsung-differentiable,2019-4,Kim,arXiv,"@article{kim2019differentiable,
  title={Differentiable Pruning Method for Neural Networks},
  author={Kim, Jaedeok and Park, Chiyoun and Jung, Hyun-Joo and Choe, Yoonsuck},
  journal={arXiv preprint arXiv:1904.10921},
  year={2019}
}",,"pruning-filters,amc-automl-han,channel-lasso-lstsq,nvidia-taylor-pruning,autopruner","VGG-Torch-CIFAR10, ResNet-56, Plain-20, MobileNet, VGG-16, ResNet-56, VGG-16","CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, ImageNet, CIFAR-10, ImageNet",,"604,605,606,607,608,609,611"
L0-arm-binary,2019-4,Li,arXiv,"@article{li2019l_0,
  title={$ L\_0 $-ARM: Network Sparsification via Stochastic Binary Optimization},
  author={Li, Yang and Ji, Shihao},
  journal={arXiv preprint arXiv:1904.04432},
  year={2019}
}",,"hard-concrete,sparse-variational-dropout,bayesian-compression","fc300-100, LeNet-5-Caffe, WRN-28-10, WRN-28-10","MNIST, MNIST, CIFAR-10, CIFAR-100",,"600,601,602,603"
lottery-ticket-followup,2019-3,Frankle,arXiv,"@article{frankle2019lottery,
  title={The Lottery Ticket Hypothesis at Scale},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}","this is v1 of what's now called ""Stabilizing the Lottery Ticket Hypothesis"" on arXiv. The two manuscripts are quite different so we refer to v1 as a separate paper.","SNIP,rethinking-net-pruning,lottery-ticket","VGG-19, ResNet-18, LeNet-5, ResNet-50, ResNet-50, ResNet-50, ResNet-50, ResNet-50","CIFAR-10, CIFAR-10, MNIST, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"40,41,42,43,443,444,445,446"
rethinking-net-pruning,2019-3,Liu,ICLR,"@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}",,"pruning-filters,thinet-channel-norms,channel-lasso-lstsq,soft-filter-pruning,network-slimming,sss,learning-both","VGG-Torch-CIFAR10, ResNet-56, ResNet-110, ResNet-34, VGG-16, ResNet-50, VGG-19, PreResNet-164, DenseNet-40, VGG-19, PreResNet-164, DenseNet-40, VGG-A+BN-noDrop, PreResNet-110, DenseNet-BC-100, PreResNet-110, DenseNet-BC-100, ???, ResNet-56, ResNet-110, ResNet-50, ResNet-50, VGG-GAP, VGG-Tiny, ResNet-56, ResNet-110, ResNet-34, VGG-16, ResNet-50, ResNet-50, ResNet-50, ResNet-50, ResNet-50, ResNet-50, VGG-16, VGG-16, VGG-16, VGG-16, ResNet-50, ResNet-50","CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, ImageNet, ImageNet, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-100, CIFAR-100, CIFAR-100, ImageNet, CIFAR-10, CIFAR-10, CIFAR-100, CIFAR-100, CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, CIFAR-10, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,440,441,528,529,531,532,533,535,536,537,538,539,586,587,588,589,590,591,592,593"
net-trim-v2,2019-2,Aghasi,arXiv,"@article{aghasi2018fast,
  title={Fast Convex Pruning of Deep Neural Networks},
  author={Aghasi, Alireza and Abdi, Afshin and Romberg, Justin},
  journal={arXiv preprint arXiv:1806.06457},
  year={2018}
}",,learning-both,"fc1000-300-100, LeNet-5, Net-Trim-ConvNet","MNIST, MNIST, CIFAR-10",,"46,47,48"
mit-coreset-pruning,2019-2,Baykal,arXiv,"@article{baykal2018data,
  title={Data-dependent coresets for compressing neural networks with applications to generalization bounds},
  author={Baykal, Cenk and Liebenwein, Lucas and Gilitschenski, Igor and Feldman, Dan and Rus, Daniela},
  journal={arXiv preprint arXiv:1804.05345},
  year={2018}
}",,,Various FC Networks,MNIST,,49
samsung-winograd-sparse,2019-2,Choi,arXiv,"@article{choi2019jointly,
  title={Jointly Sparse Convolutional Neural Networks in Dual Spatial-Winograd Domains},
  author={Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  journal={arXiv preprint arXiv:1902.08192},
  year={2019}
}",,"learning-both,net-surgery","AlexNet, ResNet-18, AlexNet, ResNet-18","ImageNet, ImageNet, ImageNet, ImageNet",,"44,45,447,543"
google-state-of-sparsity,2019-2,Gale,arXiv,"@misc{gale2019state,
    title={The State of Sparsity in Deep Neural Networks},
    author={Trevor Gale and Erich Elsen and Sara Hooker},
    year={2019},
    eprint={1902.09574},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}",google scholar can't find this for unclear reasons...,"sparse-variational-dropout,hard-concrete,lottery-ticket,learning-both","ResNet-50, Transformer","ImageNet, WMT 2014 English-German",,"50,51"
autopruner,2019-1,Luo,arXiv,"@article{luo2018autopruner,
  title={Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference},
  author={Luo, Jian-Hao and Wu, Jianxin},
  journal={arXiv preprint arXiv:1805.08941},
  year={2018}
}",,"thinet-channel-norms,sss,runtime-neural-pruning,nvidia-taylor-pruning,pruning-filters,channel-lasso-lstsq","VGG-16, VGG-16, ResNet-50, ResNet-50","CUB200-2011, ImageNet, ImageNet, ImageNet",,"52,53,54,373"
synaptic-strength,2018-12,Lin,NIPS,"@inproceedings{lin2018synaptic,
  title={Synaptic Strength For Convolutional Neural Network},
  author={Lin, Chen and Zhong, Zhao and Wei, Wu and Yan, Junjie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10149--10158},
  year={2018}
}",,"pruning-filters,network-slimming,sss,thinet-channel-norms,learning-both,ssl","VGG-netslim, ResNet-18, DenseNet-40, ResNet-50","CIFAR-10, CIFAR-10, CIFAR-10, ImageNet",,"576,577,578,579"
pcas,2018-12,Yamamoto,arXiv,"@article{yamamoto2018pcas,
  title={Pcas: Pruning channels with attention statistics},
  author={Yamamoto, Kohei and Maeno, Kurato},
  journal={arXiv preprint arXiv:1806.05382},
  year={2018}
}",,"nisp,sss,thinet-channel-norms,soft-filter-pruning,pruning-filters,amc-automl-han,channel-lasso-lstsq,huang-prune-filters","ResNet-56, ResNet-50, VGG-16, ResNet-50, SegNet, VGG-16, VGG-16","CIFAR-10, CIFAR-100, ImageNet, ImageNet, CamVid, ImageNet, ImageNet",,"64,65,66,67,68,378,379"
balanced-sparsity,2018-12,Yao,arXiv,"@article{yao2018balanced,
  title={Balanced Sparsity for Efficient DNN Inference on GPU},
  author={Yao, Zhuliang and Cao, Shijie and Xiao, Wencong},
  journal={arXiv preprint arXiv:1811.00206},
  year={2018}
}","venue: says ""copyright AAAI"" at bottom, but not in AAAI 2019 proceedings (https://aaai.org/Library/AAAI/aaai18contents.php)
",,"VGG-16, LSTM-1500-1500, Bi-LSTM-1024","ImageNet, PTB, TIMIT",,"69,70,71"
zhuang-discriminative-channel,2018-12,Zhuang,NIPS,"@inproceedings{zhuang2018discrimination,
  title={Discrimination-aware channel pruning for deep neural networks},
  author={Zhuang, Zhuangwei and Tan, Mingkui and Zhuang, Bohan and Liu, Jing and Guo, Yong and Wu, Qingyao and Huang, Junzhou and Zhu, Jinhui},
  booktitle={Advances in Neural Information Processing Systems},
  pages={875--886},
  year={2018}
}",,"channel-lasso-lstsq,thinet-channel-norms,network-slimming","???, ResNet-56, MobileNet, MobileNet-v2, ResNet-50, FaceNet, DeepFace, ???, SphereNet-4, ResNet-18, ResNet-56, ResNet-18, ResNet-18, ResNet-50, ResNet-50, ResNet-50","CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, LFW, LFW, LFW, LFW, ImageNet, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"361,362,363,364,365,367,368,369,370,372,522,523,524,525,526,527"
lottery-ticket,2018-11,Frankle,ICLR,"@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}",,,"LeNet-300-100, Toy Convnets, VGG-16-AvgPool","MNIST, CIFAR-10, CIFAR-10",,"72,73,74"
crossbar-aware,2018-10,Liang,IEEE Access,"@article{liang2018crossbar,
  title={Crossbar-aware neural network pruning},
  author={Liang, Ling and Deng, Lei and Zeng, Yueling and Hu, Xing and Ji, Yu and Ma, Xin and Li, Guoqi and Xie, Yuan},
  journal={IEEE Access},
  volume={6},
  pages={58324--58337},
  year={2018},
  publisher={IEEE}
}",,,"VGG-16, ResNet-18, VGG8","ImageNet, ImageNet, CIFAR-10",,"596,598,599"
uiuc-coreset-pruning,2018-9,Dubey,ECCV,"'@inproceedings{dubey2018coreset,
  title={Coreset-based neural network compression},
  author={Dubey, Abhimanyu and Chatterjee, Moitreya and Ahuja, Narendra},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={454--470},
  year={2018}
}",,"memory-bounded-convnets,exploit-linear-structure,learning-both,net-surgery,compression-aware-training","AlexNet, VGG-16, AlexNet, AlexNet, VGG-16, VGG-16, LeNet-5, SqueezeNet, SqueezeNet, SqueezeNet, ResNet-18, ResNet-50, ResNet-101, ResNet-18, ResNet-18, ResNet-50, ResNet-50, ResNet-101, ResNet-101, VGG-16, VGG-16","ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, MNIST, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, CUB200-2011, Stanford-Dogs",,"371,544,545,546,548,549,550,551,552,553,567,568,569,570,571,572,573,574,575,594,595"
sss,2018-9,Huang,ECCV,"'@inproceedings{huang2018data,
  title={Data-driven sparse structure selection for deep neural networks},
  author={Huang, Zehao and Wang, Naiyan},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={304--320},
  year={2018}
}",,"thinet-channel-norms,ssl,pruning-filters,channel-lasso-lstsq,nvidia-taylor-pruning,rethinking-smaller-norm","VGG-16+BN-OneFC, ResNet-20, PreResNet-164, ResNext-20, ResNext-164, ResNet-50, ResNext-50, PeleeNet, VGG-16, ResNet-50, ResNet-50","CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"75,76,77,78,79,85,86,87,380,381,382"
sparse-evolutionary,2018-9,Mocanu,Nature Communications,"@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={2383},
  year={2018},
  publisher={Nature Publishing Group}
}",,,"Family of RBMs, Family of RBMs, Family of RBMs, Family of RBMs, Family of RBMs, Family of RBMs, Family of RBMs, Family of RBMs, Family of RBMs, Family of RBMs, Family of RBMs, fc1000-1000-1000, fc4000-1000-4000, fc1000-1000-1000, Family of CNNs","UCI-Adult, UCI-Connect4, UCI-DNA, UCI-Mushrooms, UCI-NIPS-0-12, UCI-OCR-letters, UCI-RCV1, UCI-Web, Caltech101-Silhouettes-16x16, Caltech-101-Silhouettes-28x28, MNIST, MNIST, CIFAR-10, HIGGS, CIFAR-10",,"207,208,209,210,211,212,213,214,215,216,217,218,219,220,221"
extreme-net-compress,2018-9,Peng,ECCV,"'@inproceedings{peng2018extreme,
  title={Extreme network compression via filter group approximation},
  author={Peng, Bo and Tan, Wenming and Li, Zheyang and Zhang, Shun and Xie, Di and Pu, Shiliang},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={300--316},
  year={2018}
}",,"network-slimming,pruning-filters,zhang-accel-very-deep,jaderberg-low-rank-conv,channel-lasso-lstsq,samsung-vbmf-tucker","VGG-Torch-CIFAR10-noDrop, VGG-16, ResNet-34, VGG-16, VGG-16, VGG-16, ResNet-34, ResNet-34, ResNet-34","CIFAR-100, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"61,62,63,375,376,377,540,541,542"
apple-pfa,2018-9,Suau,OpenReview,"@article{suau2018network,
  title={NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES},
  author={Suau, Xavier and Zappella, Luca and Apostoloff, Nicholas},
  year={2018}
}","regarding venue, only rejected from ICLR; not uploaded to arxiv","pruning-filters,network-slimming,extreme-net-compress,dai-info-bottleneck","VGG-Torch-CIFAR10, ResNet-56, SimpleCNN, VGG-Torch-CIFAR10, ResNet-56, SimpleCNN, VGG-Torch-CIFAR10, ResNet-56, VGG-Torch-CIFAR10, ResNet-56","CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-100, CIFAR-100, CIFAR-100, CIFAR-10, CIFAR-10, CIFAR-100, CIFAR-100",,"89,90,91,92,93,94,487,488,489,490"
soft-filter-pruning,2018-8,"He, Yang",IJCAI,"@inproceedings{he2018soft,
  title={Soft filter pruning for accelerating deep convolutional neural networks},
  author={He, Y and Kang, G and Dong, X and Fu, Y and Yang, Y},
  booktitle={IJCAI International Joint Conference on Artificial Intelligence},
  year={2018}
}",,"more-is-less,pruning-filters,channel-lasso-lstsq","ResNet-20, ResNet-32, ResNet-56, ResNet-110, ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-20, ResNet-20, ResNet-32, ResNet-32, ResNet-56, ResNet-56, ResNet-56, ResNet-56, ResNet-56, ResNet-110, ResNet-110, ResNet-110, ResNet-50, ResNet-101","CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, ImageNet",,"184,185,181,192,193,194,195,196,498,499,500,501,502,503,504,505,506,507,508,509,510,511"
amc-automl-han,2018-8,"He, Yihui",ECCV,"'@inproceedings{he2018amc,
  title={Amc: Automl for model compression and acceleration on mobile devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={784--800},
  year={2018}
}",,"pruning-filters,runtime-neural-pruning,channel-lasso-lstsq","Plain-20, ResNet-56, ResNet-50, VGG-16, MobileNet, MobileNet-v2, MobileNet, MobileNet","CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"326,327,328,329,330,331,431,432"
spectral-pruning,2018-8,Suzuki,arXiv,"@article{suzuki2018spectral,
  title={Spectral-Pruning: Compressing deep neural network via spectral analysis},
  author={Suzuki, Taiji and Abe, Hiroshi and Murata, Tomoya and Horiuchi, Shingo and Ito, Kotaro and Wachi, Tokuma and Hirai, So and Yukishima, Masatoshi and Nishimura, Tomoaki},
  journal={arXiv preprint arXiv:1808.08558},
  year={2018}
}",,"net-trim,net-trimming-apoz,thinet-channel-norms,learning-both","fc300-1000-300, fc300-1000-300, VGG-16, VGG-16, VGG-16, VGG-16, VGG-16, VGG-16, VGG-16, VGG-16, VGG-16","MNIST, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"95,96,97,384,385,386,387,388,389,390,391"
learning-compression,2018-6,Carreira-Perpinan,CVPR,"@inproceedings{carreira2018learning,
  title={“Learning-Compression” Algorithms for Neural Net Pruning},
  author={Carreira-Perpin{\'a}n, Miguel A and Idelbayev, Yerlan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8532--8541},
  year={2018}
}",,learning-both,"LeNet-300-100, LeNet-5, ResNet-32, ResNet-110, ResNet-56, ResNet-32, ResNet-32, ResNet-32, ResNet-56, ResNet-56, ResNet-56, ResNet-110, ResNet-110, ResNet-110","MNIST, MNIST, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10",,"284,285,286,287,512,513,514,515,516,517,518,519,520,521"
smallify,2018-6,Leclerc,arXiv,"@article{leclerc2018smallify,
  title={Smallify: Learning Network Size while Training},
  author={Leclerc, Guillaume and Vartak, Manasi and Fernandez, Raul Castro and Kraska, Tim and Madden, Samuel},
  journal={arXiv preprint arXiv:1806.03723},
  year={2018}
}",,,"VGG-16+Dropout, fc50-50-20","CIFAR-10, Covertype",,"98,99"
nisp,2018-6,Yu,CVPR,"@inproceedings{yu2018nisp,
  title={Nisp: Pruning networks using neuron importance score propagation},
  author={Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9194--9203},
  year={2018}
}",,"perforated-cnns,pruning-filters,thinet-channel-norms,samsung-vbmf-tucker","LeNet-5, Cifar-net, AlexNet, GoogLeNet, AlexNet, ResNet-56, ResNet-110, ResNet-34, ResNet-50, AlexNet, AlexNet, ResNet-34, ResNet-50","MNIST, CIFAR-10, ImageNet, ImageNet, ImageNet, CIFAR-10, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"197,198,199,200,201,203,204,205,206,410,411,412,413"
dai-info-bottleneck,2018-4,Dai,arXiv,"@article{dai2018compressing,
  title={Compressing neural networks using the variational information bottleneck},
  author={Dai, Bin and Zhu, Chen and Wipf, David},
  journal={arXiv preprint arXiv:1802.10399},
  year={2018}
}",,"babu-generalized-dropout,ssl,sparse-variational-dropout,bayesian-compression,hard-concrete,runtime-neural-pruning,pruning-filters,network-slimming,structured-log-normal,pan-dropneuron","LeNet-300-100, LeNet-5-Caffe, VGG-16-fc512-512, VGG-Torch-CIFAR10, VGG-netslim, VGG-16-fc512","MNIST, MNIST, CIFAR-10, CIFAR-100, CIFAR-10, CIFAR-10",,"296,297,308,309,315,316"
huang-prune-filters,2018-3,Huang,WACV,"'@inproceedings{huang2018learning,
  title={Learning to prune filters in convolutional neural networks},
  author={Huang, Qiangui and Zhou, Kevin and You, Suya and Neumann, Ulrich},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={709--718},
  year={2018},
  organization={IEEE}
}",,pruning-filters,"VGG-16, ResNet-18, FCN-32s, SegNet, VGG-16, VGG-16, VGG-16, VGG-16, ResNet-18, ResNet-18, ResNet-18","CIFAR-10, CIFAR-10, Pascal VOC 2007, CamVid, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10",,"288,289,290,291,300,301,302,303,304,305,306"
ding-auto-balanced,2018-2,Ding,AAAI,"@inproceedings{ding2018auto,
  title={Auto-balanced filter pruning for efficient convolutional neural networks},
  author={Ding, Xiaohan and Ding, Guiguang and Han, Jungong and Tang, Sheng},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}",,pruning-filters,"LeNet-5-Caffe, VGG-Torch-CIFAR10, VGG-Torch-CIFAR10, ResNet-56, ResNet-56, ResNet-56","MNIST, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10",,"556,561,563,558,564,565"
rethinking-smaller-norm,2018-2,Ye,ICLR,"@article{ye2018rethinking,
  title={Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers},
  author={Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z},
  journal={arXiv preprint arXiv:1802.00124},
  year={2018}
}",,"sss,pruning-filters","conv96-192-192-384, ResNet-101, ???","CIFAR-10, ImageNet, Various Segmentation Datasets",,"267,268,276"
lstm-group-lasso,2018-1,Wen,arXiv,"@article{wen2017learning,
  title={Learning intrinsic sparse structures within long short-term memory},
  author={Wen, Wei and He, Yuxiong and Rajbhandari, Samyam and Zhang, Minjia and Wang, Wenhan and Liu, Fang and Hu, Bin and Chen, Yiran and Li, Hai},
  journal={arXiv preprint arXiv:1709.05027},
  year={2017}
}",,,"BiDaF, Custom stacked LSTMs, Variational RHN + WT","SQuaD, PTB, SQuaD",,"100,101,105"
net-trim,2017-12,Aghasi,NIPS,"@inproceedings{aghasi2017net,
  title={Net-trim: Convex pruning of deep neural networks with performance guarantee},
  author={Aghasi, Alireza and Abdi, Afshin and Nguyen, Nam and Romberg, Justin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3177--3186},
  year={2017}
}",,learning-both,"fc300-300, fc300-500-300, fc300-1000-300, fc300-300, fc300-500-300, fc300-1000-300, ???","MNIST, MNIST, MNIST, MNIST, MNIST, MNIST, MNIST",,"111,112,113,114,115,116,117"
compression-aware-training,2017-12,Alvarez,NIPS,"@inproceedings{alvarez2017compression,
  title={Compression-aware training of deep networks},
  author={Alvarez, Jose M and Salzmann, Mathieu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={856--867},
  year={2017}
}",,"exploit-linear-structure,learning-num-neurons","Dec-3-512, Dec-8-256, ResNet-50, ResNet-50, Dec-8-512","ICDAR, ICDAR, ICDAR, ImageNet, ImageNet",,"547,554,555,612,614"
openai-block-sparse,2017-12,Gray,arXiv,"@article{gray2017gpu,
  title={Gpu kernels for block-sparse weights},
  author={Gray, Scott and Radford, Alec and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1711.09224},
  year={2017}
}",,,"PixelCNN++, Small-World LSTM, Small-World LSTM, Small-World LSTM, Small-World LSTM","???, Amazon Reviews, Stanford Sentiment Treebank, IMDB, Yelp",,"106,107,108,109,110"
runtime-neural-pruning,2017-12,Lin,NIPS,"@inproceedings{lin2017runtime,
  title={Runtime neural pruning},
  author={Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2181--2191},
  year={2017}
}",,"jaderberg-low-rank-conv,pruning-filters,zhang-accel-very-deep","conv32-32-64+fc-?-?, VGG-16, VGG-16, VGG-16, VGG-16","LFW, ImageNet, ImageNet, ImageNet, ImageNet",,"270,271,272,265,266"
hard-concrete,2017-12,Louizos,NIPS,"@article{louizos2017learning,
  title={Learning Sparse Neural Networks through $ L\_0 $ Regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1712.01312},
  year={2017}
}",,"bayesian-compression,sparse-variational-dropout,ssl,structured-log-normal","LeNet-300-100, LeNet-5-Caffe+800, WRN-28-10, WRN-28-10","MNIST, MNIST, CIFAR-10, CIFAR-100",,"118,119,120,121"
bayesian-compression,2017-12,Louizos,NIPS,"@inproceedings{louizos2017bayesian,
  title={Bayesian compression for deep learning},
  author={Louizos, Christos and Ullrich, Karen and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3288--3298},
  year={2017}
}",,"sparse-variational-dropout,babu-generalized-dropout,ssl","LeNet-300-100, LeNet-5-Caffe, VGG-Torch-CIFAR10","MNIST, MNIST, CIFAR-10",,"122,123,124"
structured-log-normal,2017-12,Neklyudov,NIPS,"@inproceedings{neklyudov2017structured,
  title={Structured bayesian pruning via log-normal multiplicative noise},
  author={Neklyudov, Kirill and Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry P},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6775--6784},
  year={2017}
}",,sparse-variational-dropout,"Lenet-500-300, LeNet-5-Caffe, VGG-Torch-CIFAR10","MNIST, MNIST, CIFAR-10",,"334,335,336"
block-sparse-rnns,2017-11,Narang,arXiv,"@article{narang2017block,
  title={Block-sparse recurrent neural networks},
  author={Narang, Sharan and Undersander, Eric and Diamos, Gregory},
  journal={arXiv preprint arXiv:1711.02782},
  year={2017}
}",,,"RNN Dense 1760, GRU Dense 2560","(Proprietary), (Proprietary)",,"125,126"
channel-lasso-lstsq,2017-8,He,CVPR,"@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1389--1397},
  year={2017}
}",,pruning-filters,"ResNet-56, VGG-16, VGG-16, ResNet-50, ResNet-50, Resnet-50-DWSep, Resnet-50-DWSep, VGG-16, VGG-16, VGG-16, VGG-16","CIFAR-10, ImageNet, Pascal VOC 2007, CIFAR-10, ImageNet, CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"127,128,129,130,131,132,133,393,394,395,396"
network-slimming,2017-8,Liu,CVPR,"@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2736--2744},
  year={2017}
}",,"pruning-filters,ssl","VGG-A+BN-noDrop, PreResNet-164, VGG-netslim, VGG-netslim, PreResNet-164, fc500-300-100, VGG-netslim, PreResNet-164, DenseNet-40, DenseNet-40, DenseNet-40","ImageNet, CIFAR-10, CIFAR-10, SVHN, SVHN, MNIST, CIFAR-100, CIFAR-100, CIFAR-10, CIFAR-100, SVHN",,"55,56,57,58,59,60,80,81,82,83,84"
more-is-less,2017-7,Dong,CVPR,"@inproceedings{dong2017more,
  title={More is less: A more complicated network with less inference complexity},
  author={Dong, Xuanyi and Huang, Junshi and Yang, Yi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5840--5848},
  year={2017}
}",,,"PreResNet-110, PreResNet-164, WRN-22-8, WRN-28-2, WRN-40-1, WRN-40-2, WRN-40-4, WRN-52-1, PreResNet-164, WRN-16-4, WRN-22-8, WRN-40-1, WRN-40-2, WRN-40-4, WRN-52-1, ResNet-18, ResNet-34, ResNet-20, ResNet-32, ResNet-44","CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-10, CIFAR-100, CIFAR-100, CIFAR-100, CIFAR-100, CIFAR-100, CIFAR-100, CIFAR-100, ImageNet, ImageNet, ???, ???, ???",,"222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,273,274,275"
thinet-channel-norms,2017-7,Luo,CVPR,"@inproceedings{luo2017thinet,
  title={Thinet: A filter level pruning method for deep neural network compression},
  author={Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5058--5066},
  year={2017}
}",,"net-trimming-apoz,pruning-filters","VGG-16-AvgPool, VGG-16, ResNet-50, VGG-16, AlexNet, VGG-16, AlexNet, VGG-GAP, ResNet-50, ResNet-50, VGG-Tiny","CUB200-2011, ImageNet, ImageNet, CUB200-2011, CUB200-2011, Indoor-67-balanced, Indoor-67-balanced, ImageNet, ImageNet, ImageNet, ImageNet",,"134,135,136,137,138,139,140,397,438,439,530"
sze-energy-aware,2017-7,Yang,CVPR,"@inproceedings{yang2017designing,
  title={Designing energy-efficient convolutional neural networks using energy-aware pruning},
  author={Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5687--5695},
  year={2017}
}",,learning-both,"AlexNet, GoogLeNet, SqueezeNet","ImageNet, ImageNet, ImageNet",,"102,103,104"
sparse-variational-dropout,2017-6,"Molchanov, Dmitry",ICML,"@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2498--2507},
  year={2017},
  organization={JMLR. org}
}",,"learning-both,net-surgery,welling-slow-quantize+prune","VGG-Torch-CIFAR10, VGG-Torch-CIFAR10, LeNet-300-100, LeNet-5-Caffe","CIFAR-10, CIFAR-100, MNIST, MNIST",,"176,177,178,179"
babu-training-sparse,2017-6,Srinivas,CVPR Workshops,"@inproceedings{srinivas2017training,
  title={Training sparse neural networks},
  author={Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={138--145},
  year={2017}
}",,"babu-data-free-pruning,memory-bounded-convnets,learning-both","LeNet-5, AlexNet, VGG-16","MNIST, ImageNet, ImageNet",,"323,324,325"
nvidia-taylor-pruning,2017-4,"Molchanov, Pavlo",ICLR,"@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}",,"net-trimming-apoz,optimal-brain-damage","CaffeNet, VGG-16, R3DCNN","Flowers-102, ImageNet, nvGesture",,"243,244,245"
pruning-filters,2017-3,Li,ICLR,"@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}",,face-prune,"VGG-Torch-CIFAR10, ResNet-56, ResNet-110, ResNet-34, ResNet-56, ResNet-110, ResNet-34, ResNet-34","CIFAR-10, CIFAR-10, CIFAR-10, ImageNet, CIFAR-10, CIFAR-10, ImageNet, ImageNet",,"141,142,143,145,494,495,496,497"
google-interchannel,2017-2,Changpinyo,arXiv,"@article{changpinyo2017power,
  title={The power of sparsity in convolutional neural networks},
  author={Changpinyo, Soravit and Sandler, Mark and Zhmoginov, Andrey},
  journal={arXiv preprint arXiv:1702.06257},
  year={2017}
}",,,"TF-Example-MNIST, TF-Example-CIFAR10, Inception-v3, VGG-16n","MNIST, CIFAR-10, ImageNet, ImageNet",,"146,147,148,149"
welling-slow-quantize+prune,2017-2,Ullrich,ICLR,"@article{ullrich2017soft,
  title={Soft weight-sharing for neural network compression},
  author={Ullrich, Karen and Meeds, Edward and Welling, Max},
  journal={arXiv preprint arXiv:1702.04008},
  year={2017}
}",,"learning-both,net-surgery","LeNet-300-100, LeNet-5-Caffe, WRN-16-4, WRN-16-4","MNIST, MNIST, CIFAR-10, CIFAR-100",,"254,247,255,259"
learning-num-neurons,2016-12,Alvarez,NIPS,"@inproceedings{alvarez2016learning,
  title={Learning the number of neurons in deep networks},
  author={Alvarez, Jose M and Salzmann, Mathieu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2270--2278},
  year={2016}
}","they apparently report on places2-401 in their supplementary materials, but link to sup materials is broken on nips website: https://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks-supplemental.zip
",,"BNetC, Dec8, Dec3","ImageNet, ImageNet, ICDAR",,"251,252,253"
perforated-cnns,2016-12,Figurnov,NIPS,"@inproceedings{figurnov2016perforatedcnns,
  title={Perforatedcnns: Acceleration through elimination of redundant convolutions},
  author={Figurnov, Mikhail and Ibraimova, Aizhan and Vetrov, Dmitry P and Kohli, Pushmeet},
  booktitle={Advances in Neural Information Processing Systems},
  pages={947--955},
  year={2016}
}",,zhang-accel-very-deep,"NIN, AlexNet, VGG-16, AlexNet, AlexNet, AlexNet, AlexNet, AlexNet, VGG-16, VGG-16, VGG-16, VGG-16, VGG-16","CIFAR-10, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"240,241,242,414,415,416,417,418,419,420,421,422,423"
net-surgery,2016-12,Guo,NIPS,"@inproceedings{guo2016dynamic,
  title={Dynamic network surgery for efficient dnns},
  author={Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  booktitle={Advances In Neural Information Processing Systems},
  pages={1379--1387},
  year={2016}
}",,learning-both,"LeNet-300-100, LeNet-5-Caffe, CaffeNet","MNIST, MNIST, ImageNet",,"163,164,158"
ssl,2016-12,Wen,NIPS,"@inproceedings{wen2016learning,
  title={Learning structured sparsity in deep neural networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in neural information processing systems},
  pages={2074--2082},
  year={2016}
}",,learning-both,"fc500-300-100, LeNet-5-Caffe, CaffeNet, Cifar-net, ResNet-20, fc500-300, fc294-166, fc174-78, CaffeNet, CaffeNet, CaffeNet, CaffeNet","MNIST, MNIST, ImageNet, CIFAR-10, CIFAR-10, MNIST, MNIST, MNIST, ImageNet, ImageNet, ImageNet, ImageNet",,"153,154,155,156,157,189,190,191,482,484,485,486"
babu-generalized-dropout,2016-11,Srinivas,arXiv,"@article{srinivas2016generalized,
  title={Generalized dropout},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1611.06791},
  year={2016}
}",doesn't really seem to be about pruning; maybe exclude from spreadsheets,babu-learning-architecture,"LeNet-5-Caffe, ResNet-32, ResNet-56, GenericNet","MNIST, CIFAR-10, CIFAR-10, CIFAR-10",,"318,317,319,320"
course-pruning,2016-10,Anwar,arXiv,"@article{anwar2016compact,
  title={Compact deep convolutional neural networks with coarse pruning},
  author={Anwar, Sajid and Sung, Wonyong},
  journal={arXiv preprint arXiv:1610.09639},
  year={2016}
}",,,"CNN_small, CNN_large, CNN_medium","CIFAR-10, CIFAR-10, SVHN",,"165,166,159"
babu-learning-architecture,2016-8,Srinivas,BMVC,"@article{srinivas2015learning,
  title={Learning neural network architectures using backpropagation},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1511.05497},
  year={2015}
}","previously titled ""Learning Neural Network Architectures using Backpropagation"" according to arXiv",babu-data-free-pruning,"LeNet-5-Caffe, CaffeNet, CaffeNet, CaffeNet","MNIST, ImageNet, ImageNet, ImageNet",,"321,322,429,430"
net-trimming-apoz,2016-7,Hu,arXiv,"@article{hu2016network,
  title={Network trimming: A data-driven neuron pruning approach towards efficient deep architectures},
  author={Hu, Hengyuan and Peng, Rui and Tai, Yu-Wing and Tang, Chi-Keung},
  journal={arXiv preprint arXiv:1607.03250},
  year={2016}
}",,,"LeNet-5, VGG-16, VGG-16, VGG-16, VGG-16, VGG-16","MNIST, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"160,161,398,399,400,401"
pan-dropneuron,2016-7,Pan,arXiv,"@article{pan2016dropneuron,
  title={Dropneuron: Simplifying the structure of deep neural networks},
  author={Pan, Wei and Dong, Hao and Guo, Yike},
  journal={arXiv preprint arXiv:1606.07326},
  year={2016}
}","venue: despite what the footer says, pretty sure this didn't actually get published in nips (there doesn't seem to be a published version)",,"autoenc-fc128-64-128, LeNet-5","MNIST, MNIST",,"332,333"
group-sparse-dnns,2016-7,Scardapane,Neurocomputing,"@article{scardapane2017group,
  title={Group sparse regularization for deep neural networks},
  author={Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
  journal={Neurocomputing},
  volume={241},
  pages={81--89},
  year={2017},
  publisher={Elsevier}
}",,,"fc40-40-30-11, fc400-300-100-10, fc50-50-20-7, fc40-20","SSD, MNIST, Covertype, Digits",,"180,182,183,186"
lempitsky-fast-convnets,2016-6,Lebedev,CVPR,"@inproceedings{lebedev2016fast,
  title={Fast convnets using group-wise brain damage},
  author={Lebedev, Vadim and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2554--2564},
  year={2016}
}","pretty sure we need to not compare their numerical caffenet results to others, because the ""densities"" they report are weighted combos of fractions of nnz for different layers, weighted by forward propagation time (""When reporting the final density in sub- tasks (ii) and (iii), we weigh the densities in different layers by the forward propagation times.""). Which also suggests that their speedups are timed, not theoretical","exploit-linear-structure,lempitsky-cp-decomp,jaderberg-low-rank-conv","LeNet-5, VGG-19, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet","MNIST, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"150,152,464,465,466,467,468,469,470,471,472,480,473,474,475,476,477,478,479"
samsung-vbmf-tucker,2016-4,Kim,ICLR,"@article{kim2015compression,
  title={Compression of deep convolutional neural networks for fast and low power mobile applications},
  author={Kim, Yong-Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
  journal={arXiv preprint arXiv:1511.06530},
  year={2015}
}",,zhang-accel-very-deep,"???, VGG-16, CaffeNet, GoogLeNet","ImageNet, ImageNet, ImageNet, ImageNet",checked,"246,248,249,250"
divnet,2016-3,Mariet,ICLR,"@article{mariet2015diversity,
  title={Diversity networks: Neural network compression using determinantal point processes},
  author={Mariet, Zelda and Sra, Suvrit},
  journal={arXiv preprint arXiv:1511.05077},
  year={2015}
}",,he-reshaping,"fc500-500-sigmoid, fc500-500-sigmoid, fc3072-1000-1000-1000-sigmoid","MNIST, MNIST-ROT, CIFAR-10",,"162,167,168"
han-prune-quant-huff,2016-2,Han,ICLR,"@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}",,learning-both,"CaffeNet, VGG-16, LeNet-300-100, LeNet-5","ImageNet, ImageNet, MNIST, MNIST",,"262,263,264,269"
learning-both,2015-12,Han,NIPS,"@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}",,"exploit-linear-structure,babu-data-free-pruning,memory-bounded-convnets","LeNet-300-100, LeNet-5, CaffeNet, VGG-16, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet, CaffeNet","MNIST, MNIST, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet, ImageNet",,"170,171,172,173,402,404,405,406,407,408,409"
zhang-accel-very-deep,2015-11,Zhang,TPAMI,"@article{zhang2015accelerating,
  title={Accelerating very deep convolutional networks for classification and detection},
  author={Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={10},
  pages={1943--1955},
  year={2015},
  publisher={IEEE}
}",,jaderberg-low-rank-conv,"SPP-10, VGG-16, AlexNet, VGG-16, VGG-16, VGG-16","ImageNet, ImageNet, ImageNet, Pascal VOC 2007, ImageNet, ImageNet",checked,"280,281,282,283,582,583"
face-prune,2015-8,Polyak,IEEE Access,"@article{polyak2015channel,
  title={Channel-level acceleration of deep face representations},
  author={Polyak, Adam and Wolf, Lior},
  journal={IEEE Access},
  volume={3},
  pages={2163--2175},
  year={2015},
  publisher={IEEE}
}",,zhang-accel-very-deep,Yi's Face Network,CASIA-WebFace,,239
babu-data-free-pruning,2015-7,Srinivas,BMVC,"@article{srinivas2015data,
  title={Data-free parameter pruning for deep neural networks},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1507.06149},
  year={2015}
}",,"optimal-brain-damage,optimal-brain-surgeon","LeNet-5-Caffe, CaffeNet, CaffeNet, CaffeNet","MNIST, ImageNet, ImageNet, ImageNet",,"312,298,427,428"
liu-sparse-conv,2015-6,Liu,CVPR,"@inproceedings{liu2015sparse,
  title={Sparse convolutional neural networks},
  author={Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Pensky, Marianna},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={806--814},
  year={2015}
}",,exploit-linear-structure,CaffeNet,ImageNet,,169
lempitsky-cp-decomp,2015-4,Lebedev,ICLR,"@article{lebedev2014speeding,
  title={Speeding-up convolutional neural networks using fine-tuned cp-decomposition},
  author={Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1412.6553},
  year={2014}
}",,jaderberg-low-rank-conv,"CharNet, CaffeNet","???, ImageNet",checked,"277,278"
memory-bounded-convnets,2014-12,Collins,arXiv,"@article{collins2014memory,
  title={Memory bounded deep convolutional networks},
  author={Collins, Maxwell D and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1412.1442},
  year={2014}
}",,,"LeNet-5-Caffe, CIFAR-10 Quick, CaffeNet","MNIST, CIFAR-10, ImageNet",,"337,338,339"
exploit-linear-structure,2014-12,Denton,NIPS,"@inproceedings{denton2014exploiting,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  booktitle={Advances in neural information processing systems},
  pages={1269--1277},
  year={2014}
}",,,AlexNet,ImageNet,checked,174
he-reshaping,2014-5,He,ICASSP,"'@inproceedings{he2014reshaping,
  title={Reshaping deep neural network for fast decoding by node-pruning},
  author={He, Tianxing and Fan, Yuchen and Qian, Yanmin and Tan, Tian and Yu, Kai},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={245--249},
  year={2014},
  organization={IEEE}
}",,,"L5-1024, fc2048x7","TIMIT, Switchboard English",,"310,311"
jaderberg-low-rank-conv,2014-5,Jaderberg,BMVC,"@inproceedings{jaderberg2014speeding,
  title={Speeding up Convolutional Neural Networks with Low Rank Expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={Proceedings of the British Machine Vision Conference. BMVA Press},
  year={2014}
}",,,conv48-64-128-37,Combined Char Recognition Dataset,checked,279
early-brain-damage,1997-12,Tresp,NIPS,"@inproceedings{tresp1997early,
  title={Early brain damage},
  author={Tresp, Volker and Neuneier, Ralph and Zimmermann, Hans-Georg},
  booktitle={Advances in neural information processing systems},
  pages={669--675},
  year={1997}
}",,optimal-brain-damage,"fc10, fc5, fc3","UCI-BreastCancer, UCI-Diabetes, UCI-Boston",,"340,341,342"
optimal-brain-surgeon,1993-4,Hassibi,ICNN,"@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}",,"optimal-brain-damage,learning-both","fc3, fc3, fc3, ???","MONK1, MONK2, MONK3, ???",,"343,344,345,346"
optimal-brain-damage,1990-12,Lecun,NIPS,"@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}",,,???,1990 Handwritten Digits,,175